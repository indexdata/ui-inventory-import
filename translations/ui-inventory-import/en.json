{
    "meta.title": "Inventory import",

    "no-value": "-",
    "error": "Something went wrong",
    "selectValue": "Select a value",
    "fillIn": "Please fill this in to continue",
    "invalidJSON": "Invalid JSON: {error}",
    "invalidXML": "Invalid XML: {error}",
    "invalidXSLT": "Valid XML, but not an XSLT stylesheet",
    "validXSLT": "Valid XSLT stylesheet",
    "selectToContinue": "Please select to continue",
    "resultCount": "{count, number} {count, plural, one {record found} other {records found}}",
    "set-to-xml2json": "Choose XML-to-JSON",
    "reports": "Reports",
    "export-csv": "Export CSV",
    "transform": "Transform",
    "settings.storage": "Storage engines",
    "settings.storage.heading": "Manage storage engines",
    "settings.pipeline": "Transformation pipelines",
    "settings.pipeline.heading": "Manage transformation pipelines",
    "settings.step": "Transformation steps",
    "settings.step.heading": "Manage transformation steps",
    "settings.logs": "Logs",
    "settings.logs.heading": "Manage log deletion threshold",
    "settings.logs.description": "Logs older than the configured age threshold are automatically deleted once per day, at 2am Central European time. The threshhold is specified by a number and a unit. ",
    "settings.logs.number": "Number",
    "settings.logs.unit": "Unit",
    "settings.logs.unit.days": "Days",
    "settings.logs.unit.weeks": "Weeks",
    "settings.logs.unit.months": "Months",

    "add": "Add",
    "actions.new": "New record",
    "actions.new.harvestable.oaiPmh": "OAI-PMH harvestable",
    "actions.new.harvestable.xmlBulk": "XML bulk harvestable",
    "actions.new.harvestable.connector": "Connector harvestable",
    "actions.new.harvestable.status": "Status-report harvestable",

    "nav.harvestables": "Harvestables",
    "nav.jobs": "Jobs",
    "nav.jobs-for": "Jobs for {name}",
    "nav.records": "Failed records",
    "nav.mike": "Mike",

    "button.edit": "Edit",
    "button.delete": "Delete",
    "button.view-log": "View log",
    "button.view-log.current": "Current log",
    "button.view-log.last": "Last log",
    "button.old-jobs": "Old jobs",
    "button.start-job": "Start job",
    "button.stop-job": "Stop job",
    "button.confirm": "Confirm",
    "op.delete.confirm": "Are you sure you want to delete this record?",
    "op.delete.completed": "Record <b>{name}</b> deleted",
    "op.run.completed": "Job <b>{name}</b> started",
    "op.stop.completed": "Job <b>{name}</b> stopped",
    "op.run.error": "Could not start job <b>{name}</b>: {error}",
    "op.stop.error": "Could not stop job <b>{name}</b>: {error}",

    "harvestables.column.name": "Name",
    "harvestables.column.currentStatus": "Status",
    "harvestables.column.records": "Records",
    "harvestables.column.lastHarvestFinished": "Last harvest finished",
    "harvestables.column.enabled": "Enabled?",
    "harvestables.column.jobClass": "Job class",
    "harvestables.column.id": "ID",
    "harvestables.column.message": "Message (processed/loaded/deleted(skipped)/failed)",
    "harvestables.column.logFile": "Log file",
    "harvestables.column.oldJobs": "Old jobs",

    "harvestables.column.enabled.true": "Yes",
    "harvestables.column.enabled.false": "No",

    "harvestables.column.jobClass.OaiPmhResource": "OAI-PMH",
    "harvestables.column.jobClass.XmlBulkResource": "XML bulk",
    "harvestables.column.jobClass.HarvestConnectorResource": "Harvest connector",
    "harvestables.column.jobClass.StatusResource": "Status report",

    "harvestables.column.currentStatus.NEW": "New",
    "harvestables.column.currentStatus.OK": "OK",
    "harvestables.column.currentStatus.WARN": "Warning",
    "harvestables.column.currentStatus.ERROR": "Error",
    "harvestables.column.currentStatus.RUNNING": "Running",
    "harvestables.column.currentStatus.FINISHED": "Finished",
    "harvestables.column.currentStatus.KILLED": "Killed",
    "harvestables.column.currentStatus.SHUTDOWN": "Shutdown",

    "harvestables.index.name": "Name",
    "harvestables.index.id": "ID",
    "harvestables.index.message": "Message",

    "jobs.index.all": "(All)",
    "jobs.index.name": "Harvestable name",
    "jobs.index.id": "ID",
    "jobs.index.harvestableId": "Harvestable ID",
    "jobs.index.message": "Message",

    "records.index.all": "(All)",
    "records.index.recordNumber": "Record number",
    "records.index.instanceHrid": "Instance hrid",
    "records.index.instanceTitle": "Instance title",
    "records.index.errors": "Errors",
    "records.index.timeStamp": "Time stamp",
    "records.index.harvestableName": "Harvestable name",

    "searchInputLabel": "XXX searchInputLabel",
    "filter.date.started": "Date started",
    "filter.date.started.from": "From",
    "filter.date.started.to": "To",
    "filter.date.finished": "Date finished",
    "filter.date.finished.from": "From",
    "filter.date.finished.to": "To",
    "filter.date.timeStamp": "Time stamp",
    "filter.date.timeStamp.from": "From",
    "filter.date.timeStamp.to": "To",
    "filter.numeric.records": "Records",
    "filter.numeric.records.from": "At least",
    "filter.numeric.records.to": "No more than",

    "accordion.devinfo": "Developer information",

    "harvestables.heading.general": "General information",
    "harvestables.field.id": "Id",
    "harvestables.field.jobClass": "Job class",
    "harvestables.field.jobClass.oaiPmh": "OAI-PMH",
    "harvestables.field.jobClass.xmlBulk": "XML bulk",
    "harvestables.field.jobClass.connector": "Harvest connector",
    "harvestables.field.jobClass.status": "Status report",
    "harvestables.field.id.help": "Automatically assigned identifier for the job",
    "harvestables.field.name": "Name",
    "harvestables.field.name.help": "Preferably a unique name for users to identify this Harvester resource. In some cases the name may be proposed after filling out protocol specific section of the configuration (e.g Index Data Connectors, OAI-PMH).",
    "harvestables.field.serviceProvider": "Service provider",
    "harvestables.field.serviceProvider.help": "Free-text field used by support staff for recording administrative information. Not used by the harvester.",
    "harvestables.field.usedBy": "Used by",
    "harvestables.field.usedBy.help": "Free-text field for tagging a job with the intended target audience, like a user group or customer of the resource. Multiple user/customer tags may be separated by commas. The tags can be used for filtering status reports by usages/customers.",
    "harvestables.field.managedBy": "Managed by",
    "harvestables.field.managedBy.help": "Free-text field for tagging a job with the producer or manager of the resource. Multiple tags may be separated by commas. The tags can be used for filtering status reports by job administrators.",
    "harvestables.field.openAccess": "Open access resource?",
    "harvestables.field.description": "Content description",
    "harvestables.field.description.help": "Free-text field used by support staff for recording administrative information. Not used by the harvester.",
    "harvestables.field.technicalNotes": "Technical notes",
    "harvestables.field.technicalNotes.help": "Free-text field used by support staff for recording administrative information. Not used by the harvester.",
    "harvestables.field.contactNotes": "Contact notes",
    "harvestables.field.contactNotes.help": "Free-text field used by support staff for recording administrative information. Not used by the harvester.",
    "harvestables.field.enabled": "Harvest job enabled?",
    "harvestables.field.enabled.help": "Check to run the Harvesting job as described by the time/interval selected in \"Harvest schedule\". Leaving this box unchecked will make the job inactive.",
    "harvestables.field.scheduleString": "Harvest schedule",
    "harvestables.field.scheduleString.help": "Use these fields to define a recurring time/interval at which the Harvester job should run. E.g for weekly runs specify a day of the week on which the harvest should be executed.",
    "harvestables.field.scheduleString.harvest": "Harvest",
    "harvestables.field.scheduleString.day": "(day) of",
    "harvestables.field.scheduleString.month": "(month) if it's",
    "harvestables.field.scheduleString.weekday": "(day of the week)",
    "harvestables.field.scheduleString.time": "Harvesting time",
    "harvestables.field.scheduleString.hour": "(hour in 24 format)",
    "harvestables.field.scheduleString.minute": "(min)",
    "harvestables.field.transformationPipeline": "Transformation pipeline",
    "harvestables.field.transformationPipeline.help": "Select the transformation required to match the input format delivered by the feed to the internal format used by the Harvester for data storage. See the Transformation Pipelines manual section for more details.",
    "harvestables.field.laxParsing": "Use lax parsing (if possible)",
    "harvestables.field.laxParsing.help": "When enabled, harvester will attempt to parse malformed XML (missing closing tags, entities)",
    "harvestables.field.encoding": "Encoding override (ISO-8859-1, UTF-8, ...)",
    "harvestables.field.encoding.help": "A feed can return invalid encoded responses, such as having an XML header with encoding set to UTF-8, but actually return ISO-8859-1 in the data. Setting this field to the actual encoding will force the Harvester to use the specified encoding.",
    "harvestables.field.storage.name": "Storage",
    "harvestables.field.storage.name.help": "Select the storage type and location for the harvested data. The Harvester has a storage abstraction layer to allow it to work with multiple potential record storage systems, but at present, only Solr/Lucene is supported. Once the Storage has been selected, it is possible to view the indexed records by clicking the Stored records: click to view field.",
    "harvestables.field.storageBatchLimit": "Storage batch limit",
    "harvestables.field.cacheEnabled": "Cache on disk?",
    "harvestables.field.cacheEnabled.help": "If enabled, harvest data is kept in the filesystem cache and the job can be restarted from this cache without needing to go back to the server.",
    "harvestables.field.storeOriginal": "Store original record content?",
    "harvestables.field.recordLimit": "Limit record number to",
    "harvestables.field.recordLimit.help": "Limit the harvest run to a specified number of records: useful for testing job settings and transformation pipelines.",
    "harvestables.field.timeout": "Connection/read timeout (seconds)",
    "harvestables.field.timeout.help": "Specify a non-default timeout value for obtaining and reading from the network connection (socket). Values under 1 minute are not recommended.",
    "harvestables.field.logLevel": "Log level",
    "harvestables.field.logLevel.help": "Specify the logging level for the job with DEBUG being the most verbose. INFO is the recommended log level in most cases.",
    "harvestables.field.failedRecordsLogging": "Saving failed records",
    "harvestables.field.failedRecordsLogging.help": "Specify whether or not failed records should be saved as XML files in a designated log directory. Also specify retention policy for the directory, that is, whether to retain files that were saved in previous runs and, if so, whether to overwrite an existing file if the same record fails again or rather add a sequence number to the new file name in order not to overwrite.",
    "harvestables.field.maxSavedFailedRecordsPerRun": "Maximum number of failed records saved next run",
    "harvestables.field.maxSavedFailedRecordsPerRun.help": "Sets a maximum number of files to save in the failed records directory per run. The job log will tell when the limit is reached.",
    "harvestables.field.maxSavedFailedRecordsTotal": "Maximum number of failed records saved total",
    "harvestables.field.maxSavedFailedRecordsTotal.help": "Sets a maximum number of files to be saved in the failed records directory at any given time - as the sum of previously saved records (that were not cleaned up before this run) plus any new records added during the run. The job log will tell when the limit is reached.",
    "harvestables.field.mailAddress": "Notification e-mail addresses",
    "harvestables.field.mailAddresses": "Notification e-mail addresses",
    "harvestables.field.mailAddresses.help": "List of e-mail addresses that should receive notification on job completion.",
    "harvestables.field.mailLevel": "Send notification if severity at least",
    "harvestables.field.mailLevel.help": "specify job completion status with the least severity that will trigger the e-mail notification.",
    "harvestables.field.constantFields": "List of constant fields",
    "harvestables.field.constantFields.help": "A list of NAME=VALUE pairs. For a harvestable that has this field set, each harvested record has each NAME field set to the corresponding VALUE.",
    "harvestables.field.json": "Extra configuration: (JSON)",
    "harvestables.field.json.help": "Specify additional advanced harvester configuration in the JSON format.",

    "harvestables.field.type.oaiPmh": "OAI-PMH specific information",
    "harvestables.field.url": "OAI repository URL",
    "harvestables.field.url.help": "Enter a link (http-based) to the resource to be harvested. Include the base link defined by OAI Set Name: (see below). Some resources have multiple sets within the repository. If no specific set is identified by the URL, the full repository will be harvested.",
    "harvestables.field.oaiSetName": "OAI set name (type for suggestions)",
    "harvestables.field.oaiSetName.help": "an optional setting, an OAI-PMH setSpec value which specifies set criteria for selective harvesting.",
    "harvestables.field.metadataPrefix": "Metadata prefix",
    "harvestables.field.metadataPrefix.help": "A string that specifies the metadata format in OAI-PMH requests issued to a targeted repository. It is important to choose the correct format or no data will be harvested from the repository. Make sure a Transformation Pipeline that matches the metadata format used in the repository is selected, otherwise records will not be understood by the Harvester. Repositories generally use one of the following prefixes (or embedded data formats): Dublin Core (OAI-DC) or MARC XML (MARC12/USMARC). Other less common MetadataPrefix values include PMC (PubMed Central full-text records), PMC (PubMed Central metadata records), and PZ2 (pazpar2).",
    "harvestables.field.dateFormat": "Date format",
    "harvestables.field.useLongDateFormat": "Use long date format",
    "harvestables.field.useLongDateFormat.help": "Check-box to indicate whether to use a long date format when requesting records from the OAI-PMH resource. This is not used very often, but is required by some resources.",
    "harvestables.field.fromDate": "Harvest from",
    "harvestables.field.fromDate.help": "If empty and no resumption token is set, the Harvester will harvest the full data set from the resource. When this field contains a value, upon completion of the job the Harvester will reset the value of this field to the day prior to the current run date, so subsequent runs will harvest only new records.",
    "harvestables.field.untilDate": "Harvest until",
    "harvestables.field.untilDate.help": "Upper date limit for selective harvesting. On consecutive runs the Harvester will clear this field making the date interval open-ended.",
    "harvestables.field.resumptionToken": "Resumption token (overrrides date)",
    "harvestables.field.resumptionToken.help": "The OAI-PMH protocol supports splitting bigger datasets into smaller chunks. On delivery of a chunk of records, the OAI-PMH returns a token which the next request should use in order to get the next chunk. If an OAI-PMH job halts before completion, the resumption token will be set in this field. Sometimes it is possible to run it again from this resumption point at a later stage, but this is not always supported.",
    "harvestables.field.clearRtOnError": "Clear resumption token on connection errors",
    "harvestables.field.clearRtOnError.help": "Clear the resumption token for harvests that complete in an error state. This is useful when server errors out and the last resumption token is no longer valid.",
    "harvestables.field.keepPartial": "Keep partial harvests",
    "harvestables.field.keepPartial.help": "When checked, partial records harvested during a failed harvest run will be retained in storage.",
    "harvestables.field.retryCount": "Request retry count",
    "harvestables.field.retryCount.help": "Specify how many times the harvester should retry failed harvest requests, 0 disables retrying entirely.",
    "harvestables.field.retryWait": "Delay before retry (seconds)",
    "harvestables.field.retryWait.help": "Delay for retrying failed requests. Only change when resource fails to work with the default values.",

    "harvestables.field.type.xmlBulk": "XML bulk specific information",
    "harvestables.field.urls": "URLs",
    "harvestables.field.urls.help": "One or more space-separated URL (HTTP or FTP) for XML  or MARC binary data. Jump or index pages (HTML pages with URLs) are supported and  so are FTP directories. For FTP, harvesting of recursive directories may be enabled below.",
    "harvestables.field.allowErrors": "Continue on errors?",
    "harvestables.field.allowErrors.help": "Check to continue harvesting and storing records even if  retrieving some of the listed resources fails.",
    "harvestables.field.overwrite": "Overwrite data with each run (non-incremental)?",
    "harvestables.field.overwrite.help": "Check to delete all previously  harvested data before beginning the next scheduled (or manually triggered) run. This  may be used when complete catalog dumps reside on the server.    With FOLIO Inventory Storage there is no deletion of all previously harvested data, and checking  this option instead indicates that existing records should be overlaid.",
    "harvestables.field.allowCondReq": "Ask server for new files only (incremental)?",
    "harvestables.field.allowCondReq.help": "Ask the server if the files are  modified before attempting a harvest, relies on proper timestamp handling on the  server side. It\u2019s usually safe to have this enabled as servers are eager to  update the modification date, even in cases when the files themselves don\u2019t  change. Enabling this setting may significantly shorten harvest times.",
    "harvestables.field.initialFromDate": "Initial from date (if incremental)",
    "harvestables.field.initialFromDate.help": "Allows to specify the initial from  harvest date when <strong>ask server for new files only</strong> option is checked. When filled out, only files newer than the specified date will be harvested.    With FOLIO Inventory Storage, the setting additionally indicates that only incoming records  that were updated on or after this date should be loaded. Additionally, for this to take effect,  the incoming records must provide a 'lastUpdated' in the element 'processing' and on the format YYYY-MM-DD &lt;processing&gt; &lt;lastUpdated&gt;1970-01-01&lt;/lastUpdated&gt; &lt;/processing&gt; By default the logic would filter by the finishing date of the last harvest, so setting  'Initial from date' overrides the default behavior.   Following rules thus applies:   <li>If 'Overwrite data' is checked, all records are loaded.</li>  Otherwise:   <li>If the incoming records provide a 'lastUpdated' date, and the 'Initial from date' is set,  then only records updated at or after that date will be loaded (this is regardless of  whether 'Ask server for new files only (incremental)' is checked or not)</li> <li>If the incoming records provide a 'lastUpdated' date, and the 'Initial from date' is NOT set,  then only records updated at or after the last harvest date will be loaded</li>   But   <li>Any record without a 'lastUpdated' date will be loaded</li>",
    "harvestables.field.splitAt": "Split XML at depth (zero/empty disables split)",
    "harvestables.field.splitAt.help": "For XML data. This should  usually be set to 1 for XML feeds, if we want to harvest the record elements in  the data structured like:   &lt;root&gt; &nbsp;&lt;record/&gt; &nbsp;&lt;record/&gt; &lt;/root&gt;",
    "harvestables.field.splitSize": "Split files at number of records (zero/empty disables split)",
    "harvestables.field.splitSize.help": "The Harvester  tries to imply streaming parsing where possible, but many XSL Transformations  will not support this. Attempting to transform millions of records will be too  memory consuming, so breaking the resource into chunks of 1000 records seems to  be a reasonable option. Enter into this field the number of records to be  contained in each chunk.",
    "harvestables.field.expectedSchema": "Mime-type override (e.g: application/marc; charset=MARC-8)",
    "harvestables.field.expectedSchema.help": "The Harvester detects  the type (XML vs MARC binary) from the MIME-type and file extension. It is also able  to deal with compressed archives (zip, tar, gzip), in some rare case it may be  required to provide the content type manually (e.g if it\u2019s missing or wrong),  the format is:   MIME-type [; optional character encoding].",
    "harvestables.field.outputSchema": "MARC XML transformation format (application/marc or application/tmarc)",
    "harvestables.field.outputSchema.help": "This field  expresses the output format of binary MARC reading\u2013which will also be the input  format for the transformation pipeline. If the Transformation Pipeline expects MARC21  XML, this should be set to Application/marc. If the pipeline expects Turbo MARC XML,  it should be set to Application/tmarc.",
    "harvestables.field.recurse": "Recurse into subfolders?",
    "harvestables.field.recurse.help": "When set, the harvester will traverse the entire directory  tree and search for harvestable files. This setting should be enabled with care.",
    "harvestables.field.includeFilePattern": "Include files (regular expression)",
    "harvestables.field.includeFilePattern.help": "This setting can be used to filter what files  to harvest.  The filter applies to files in FTP directories as well as in archives (ZIP/tar).  When set to a regular expression, the harvester  will only harvest files with names matching the regular expression (unless the file name is  at the same time excluded by an exclude pattern).   Example:  <pre>.*\\.xml|.*\\.marc</pre>  Would include only .xml and .marc files.    Note that file name dots must be escaped.    Note that ZIP and tar files (.zip,.gz,.tar) are loaded even if they are not specified in the  include pattern. To enforce exclusion of ZIP or tar files they would have to be specified in  an exclude pattern (see help text for that).",
    "harvestables.field.excludeFilePattern": "Exclude files (regular expression)",
    "harvestables.field.excludeFilePattern.help": "This setting can be used to filter what files to harvest. The filter applies to files in FTP directories as well as entries in archives (ZIP/tar).  When set to a regular expression, the harvester  will skip any file with a file name matching the expression. Example: <pre>readme\\.txt|README|.*\\.jpg|.*\\.gif</pre>  Would exclude files with names readme.txt, README as well as .jpg and .gif files  from FTP directories or ZIP/tar archives.",
    "harvestables.field.passiveMode": "Use passive mode for FTP transfers?",
    "harvestables.field.passiveMode.help": "When set passive, instead of active, mode is  used for FTP connections. If harvester is running within a restricted firewall that  blocks FTP active mode connections, enabling this setting might help. It might be,  however, necessary to align this mode with what FTP server expects.",
    "harvestables.field.csvConfiguration": "CSV parser configuration",
    "harvestables.field.csvConfiguration.help": "The harvester will detect (either by MIME-type or by file extension)  and attempt to parse CSV (comma separated values) files into an XML representation for further processing. The XML representation of each  data row looks as follows: <pre>&lt;row&gt;&lt;field name=&quot;column name or number&quot;&gt;field value&lt;/field&gt;...&lt;/row&gt;</pre>  Unless the split at depth option is set to > 0, all rows will be parsed into a single XML document and wrapped with an additional &lt;rows&gt; root element. For large CSV files it may be a good idea to set the split at depth to 1.  The parser configuration is expressed in a semicolon delimited key/value list, like so: key1=value1; key2=value2. List of supported options is as follows: <ul> <li> charset: default \"iso-8859-1\", specifies the character encoding of the files</li> <li> delimiter: default \",\" for CSV and \"\\t\" for TSV, specifies the field delimiter used in the files</li> <li> containsHeader: default \"yes\", specifies if the first line in the files contains the header line</li> <li> headerLine: no default, allows to override or specify headers, format is a comma-separated list e.g headers=\"title,author,description\"</li></ul>",

    "harvestables.field.type.connector": "Connector specific information",
    "harvestables.field.connectorEngineUrlSetting.label": "CF Engine",
    "harvestables.field.connectorEngineUrlSetting.label.help": "Select the Connector Engine instance that will be used to execute  the Connector harvesting job. The default engine is hosted by Index Data but may be  also installed locally on the customer site. Additional Connector Engines can be  specified through the Settings tab.",
    "harvestables.field.engineParameters": "Engine parameters (optional)",
    "harvestables.field.engineParameters.help": "Additional or custom values of Connector Engine  session parameters used by this job. See CFWS manpage for more information.",
    "harvestables.field.connectorRepoUrlSetting.label": "CF Repository",
    "harvestables.field.connectorRepoUrlSetting.label.help": "Select the connector repository where the Connectors are hosted  and maintained. Usually, the Connector Repository is provided by Index Data and may  require a login account. The account credentials are provided directly in the  Connector Repository URL setting accessed from the Settings tab and should have the  form: <pre>http(s)://&lt;repouser&gt;:&lt;repopass&gt;@url.to.the.repository</pre>.",
    "harvestables.field.connector": "Connector (type for suggestion)",
    "harvestables.field.connector.help": "Enter here the name of the harvesting  connector specific to the harvested resource. This field provides suggestions by  looking up the Repository so only a couple of initial characters or a part of the  name is required.",
    "harvestables.field.overwrite-connector.help": "Check to delete all previously  harvested data before beginning the next scheduled (or manually triggered) run.",  
    "harvestables.field.connuser": "User Name",
    "harvestables.field.connuser.help": "User name required for access to a harvested resource that  requires authentication.",
    "harvestables.field.password": "Password",
    "harvestables.field.password.help": "Password required for access to a harvested resource that requires  authentication.",
    "harvestables.field.proxy": "Proxy server address",
    "harvestables.field.proxy.help": "Address of the proxy server that should be used by the  harvesting engine, e.g to deal with cases when the resource is IP authenticated.",
    "harvestables.field.initData": "Init Data",
    "harvestables.field.initData.help": "Advanced setting to provide additional initialization parameters  to the harvesting connector. Any username/password/proxy specified in the inputs  above will take precedence over settings specified in this field. These settings  must be provided in JSON format.",
    "harvestables.field.startToken": "Start token (incremental harvest)",
    "harvestables.field.startToken.help": "The use of a start token for incremental  harvesting is connector specific and depends on the connector capability. This setting  must be provided in JSON format.",
    "harvestables.field.sleep": "Delay between requests (milliseconds)",
    "harvestables.field.sleep.help": "Delay between requests made from the  harvester to the connector engine. Use when the resource is sensitive to high loads.",
    "harvestables.field.failedRetryCount": "Failed request retry count",
    "harvestables.field.failedRetryCount.help": "Specify how many times the harvester should retry  failed harvest requests, 0 disables retrying entirely.",

    "harvestables.field.type.status": "General information",
    "harvestables.field.usageTags": "Filter: list of usage tags",
    "harvestables.field.usageTags.help": "Used for filtering the status report by the user groups on customers tagged to a harvest job (\"Used by\").",
    "harvestables.field.adminTags": "Filter: list of admin tags",
    "harvestables.field.adminTags.help": "Used for filtering the status report by the harvestables creator or administrator, as tagged to the harvest job (\"Managed by\")",
    "harvestables.field.statusJobEnabled": "Status job enabled",
    "harvestables.field.statusJobEnabled.help": "",
    "harvestables.field.customMailAddresses": "Custom e-mails addresses (multiple separated with comma)",
    "harvestables.field.customMailAddresses.help": "",

    "harvestables.heading.status": "Status information",
    "harvestables.field.currentStatus": "Current status:",
    "harvestables.field.initiallyHarvested": "Initial harvest",
    "harvestables.field.lastHarvestStarted": "Last harvest started",
    "harvestables.field.lastHarvestFinished": "Last harvest completed",
    "harvestables.field.lastUpdated": "Last updated",
    "harvestables.field.message": "Message from last harvest:",

    "harvestables.field.failedRecordsLogging.NO_STORE": "Don't save failed records",
    "harvestables.field.failedRecordsLogging.CLEAN_DIRECTORY": "Do save. Clean up directory first",
    "harvestables.field.failedRecordsLogging.CREATE_OVERWRITE": "Do save. Overwrite existing files",
    "harvestables.field.failedRecordsLogging.ADD_ALL": "Do save. Add numbered versions for existing files",

    "storage.field.name": "Name",
    "storage.field.description": "Description",
    "storage.field.enabled": "Enabled",
    "storage.field.url": "URL",
    "storage.field.json": "JSON configuration",
    "storage.field.type": "Storage type:",

    "pipeline.field.name": "Name",
    "pipeline.field.description": "Description",
    "pipeline.field.enabled": "Enabled",
    "pipeline.field.parallel": "Parallel",
    "pipeline.field.stepAssociations": "Transformation steps",
    "pipeline.steps.position": "#",
    "pipeline.steps.name": "Name",
    "pipeline.steps.in": "In",
    "pipeline.steps.out": "Out",
    "pipeline.steps.actions": "Actions",
    "step.field.name": "Name",
    "step.field.description": "Description",
    "step.field.enabled": "Enabled (This is unused)",
    "step.field.type": "Type",
    "step.field.inputFormat": "Input format",
    "step.field.outputFormat": "Output format",
    "step.field.script": "Script",
    "step.field.testData": "Test data",
    "step.field.testOutput": "Test output",
    "step.field.customClass": "Custom class",

    "logs.plainTextLog.running": "Live plain text log for current running job",
    "logs.plainTextLog.previous": "Plain text log for last job",
    "logs.plainTextLog.refresh": "Refresh",
    "logs.plainTextLog.download": "Download",
    "logs.countFailedRecords": "{count, number} {count, plural, one {failed record} other {failed records}}",

    "summary-table.summary": "Summary",
    "summary-table.instances": "Instance",
    "summary-table.holdings": "Holdings",
    "summary-table.items": "Item",

    "summary-label.processed": "Processed",
    "summary-label.loaded": "Loaded",
    "summary-label.deleted": "Deleted(skipped)",
    "summary-label.failed": "Failed",

    "failed-records.recordNumber": "Record number",
    "failed-records.timeStamp": "Time stamp",
    "failed-records.instanceHrid": "Instance HRID",
    "failed-records.instanceTitle": "Instance title",
    "failed-records.errors": "Errors",
    "failed-records.harvestableName": "Harvestable name",

    "jobs.column.name": "Harvestable name",
    "jobs.column.status": "Status",
    "jobs.column.amountHarvested": "Records",
    "jobs.column.seconds": "Seconds",
    "jobs.column.started": "Started",
    "jobs.column.finished": "Finished",
    "jobs.column.type": "Job class",
    "jobs.column.message": "Message (processed/loaded/deleted(skipped)/failed)",

    "stats.instances": "Instances",
    "stats.holdings": "Holdings",
    "stats.items": "Items",

    "error.invalidSort.label": "Invalid sort criterion",
    "error.invalidSort.content": "It is not possible to sort on the <code>{name}</code> column due to implementation limitations.",

    "permission.settings.view": "Inventory import (Settings): View storage engines, transformation pipelines and transformation steps, and log-file deletion threshold",
    "permission.settings.view-edit": "Inventory import (Settings): View, edit storage engines, transformation pipelines and transformation steps, and log-file deletion threshold",
    "permission.settings.view-edit-create": "Inventory import (Settings): View, edit, create storage engines, transformation pipelines and transformation steps",
    "permission.settings.view-edit-create-delete": "Inventory import (Settings): View, edit, create, delete storage engines, transformation pipelines and transformation steps",
    "permission.harvestables.view": "Inventory import: View harvestables",
    "permission.harvestables.view-edit": "Inventory import: View, edit harvestables, start/stop jobs",
    "permission.harvestables.view-edit-create": "Inventory import: View, edit, create harvestables, start/stop jobs",
    "permission.harvestables.view-edit-create-delete": "Inventory import: View, edit, create, delete harvestables, start/stop jobs",
    "permission.jobs-and-failed-records.view": "Inventory import: View jobs and failed records",
    "permission.all": "Inventory import: All permissions",

    "SENTINEL": "SENTINEL"
}
